{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from belly import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def read_schema(path):\n",
    "    fs = GCSFileSystem(project='trollhunters')\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        schema = pickle.load(f)\n",
    "    return schema\n",
    "\n",
    "\n",
    "def parse_dates(obj, key, fn):\n",
    "    _parse = lambda k,v: parse_dates(v,key,fn) if k != key else fn(v)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {k:_parse(k,v) for k,v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [parse_dates(vi, key, fn) for vi in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def replace_timestamps_in_dat(dat, fn):\n",
    "    dat = parse_dates(dat, 'created_at', fn)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tweepy.utils import parse_datetime\n",
    "\n",
    "def messages_to_df(spark, schema, messages):\n",
    "    tweets = [json.loads(msg.value()) for msg in messages]\n",
    "    tweets = [replace_timestamps_in_dat(t, parse_datetime) for t in tweets]\n",
    "    tweets = [cast_coords(tw) for tw in tweets]\n",
    "    return spark.createDataFrame(tweets, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spark = build_spark()\n",
    "schema = read_schema('gs://spain-tweets/schemas/tweet-clean.pickle')\n",
    "c = get_consumer()\n",
    "\n",
    "\n",
    "\n",
    "messages = consume(c, 20000)\n",
    "\n",
    "\n",
    "# tweets = [json.loads(msg.value()) for msg in messages]\n",
    "# tweets = [replace_timestamps_in_dat(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def _cast_doubles(a):\n",
    "    if type(a) == list:\n",
    "        return [_cast_doubles(i) for i in a]\n",
    "    if type(a) == dict:\n",
    "        return {k:_cast_doubles(v) for k,v in a.items()}\n",
    "    try:\n",
    "        return float(a) \n",
    "    except ValueError:\n",
    "        return a\n",
    "    except TypeError:\n",
    "        return a\n",
    "\n",
    "\n",
    "def cast_coords(tw):\n",
    "    tw = deepcopy(tw)\n",
    "    if type(tw) != dict:\n",
    "        return tw\n",
    "\n",
    "    for k,v in tw.items():\n",
    "        if k == 'coordinates':\n",
    "            tw[k] = _cast_doubles(v)\n",
    "        else:\n",
    "            tw[k] = cast_coords(v)\n",
    "    \n",
    "    return tw\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "schema = read_schema('gs://spain-tweets/schemas/tweet-clean.pickle')\n",
    "\n",
    "df = messages_to_df(spark, schema, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "\n",
    "def dedup_data(d, week, year, inpath):\n",
    "    ids = spark.read.parquet(inpath).select('id').where(f'week = {week} and year = {year}')\n",
    "    d = d.where(f'week = {week} and year = {year}')\n",
    "    d = d.join(ids, on='id', how='left_anti')\n",
    "    return d\n",
    "\n",
    "def write_out(d, week, year, outpath):\n",
    "    f = path.join(outpath, f'year={year}', f'week={week}')\n",
    "    d.where(f'week = {week} and year = {year}').write.mode('append').parquet(f)\n",
    "\n",
    "def indempotent_write(df, warehouse):\n",
    "    df.registerTempTable('tweets')\n",
    "    dd = spark.sql('select *, weekofyear(created_at) as week, month(created_at) as month, year(created_at) as year from tweets')\n",
    "    dd.registerTempTable('tweets')\n",
    "    combos = spark.sql('select distinct year, week from tweets').collect()\n",
    "    for combo in combos:\n",
    "        week,year = combo.week, combo.year\n",
    "        d = dedup_data(dd, week, year, warehouse)\n",
    "        write_out(d, week, year, warehouse)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "indempotent_write(df, 'gs://spain-tweets/datalake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('gs://spain-tweets/datalake').where('year=2020 and week=3').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16613"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('gs://spain-tweets/datalake').where('year=2020 and week=4').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('gs://spain-tweets/datalake').where('year=2020 and week=3').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16613"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('gs://spain-tweets/datalake').where('year=2020 and week=4').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pyspark.sql.types import StructField, StructType, ArrayType\n",
    "\n",
    "def replace_with_type(schema, target, NewType):\n",
    "    if hasattr(schema, 'fields'):\n",
    "        new_struct = StructType()\n",
    "        for field in schema:\n",
    "            if field.name == target:\n",
    "                try:\n",
    "                    dt = NewType()\n",
    "                except TypeError:\n",
    "                    dt = deepcopy(NewType)\n",
    "            else:\n",
    "                dt = replace_with_type(field.dataType, target, NewType)\n",
    "                \n",
    "            new_struct.add(field.name, dt)\n",
    "        return new_struct\n",
    "    elif hasattr(schema, 'elementType'):\n",
    "        dt = replace_with_type(schema.elementType, target, NewType)\n",
    "        new_array = ArrayType(dt)\n",
    "        return new_array\n",
    "    else:\n",
    "        return schema\n",
    "\n",
    "def get_field(struct, name):\n",
    "    return [f.dataType for f in struct.fields if f.name == name][0]\n",
    "\n",
    "def set_field(struct, name, value):\n",
    "    new_struct = StructType()\n",
    "    for f in struct:\n",
    "        if f.name != name:\n",
    "            new_struct.add(f)\n",
    "\n",
    "    struct.add(name, data_type = deepcopy(value))\n",
    "    return struct\n",
    "\n",
    "\n",
    "def filter_fields(struct, fields):\n",
    "    new_struct = StructType()\n",
    "    for f in struct:\n",
    "        if f.name not in fields:\n",
    "            new_struct.add(f)\n",
    "    return new_struct\n",
    "\n",
    "def create_schema():\n",
    "    # random original schema inferred from a bunch of tweets\n",
    "    schema = read_schema('gs://spain-tweets/schemas/tweet-3.pickle')\n",
    "    user = get_field(schema, 'user')\n",
    "\n",
    "    s = deepcopy(schema)\n",
    "    s = replace_with_type(s, 'source_user', user)\n",
    "    s = replace_with_type(s, 'created_at', TimestampType)\n",
    "\n",
    "    s = filter_fields(s, ['th_original', 'th_rehydrated'])\n",
    "    s = filter_fields(s, ['retweeted_status', 'quoted_status'])\n",
    "\n",
    "    s = set_field(s, 'quoted_status', s)\n",
    "    s = set_field(s, 'retweeted_status', s)\n",
    "\n",
    "    # remove if we no longer want UB original formats\n",
    "    s = set_field(s, 'th_original', get_field(schema, 'th_original'))\n",
    "    s = set_field(s, 'th_rehydrated', get_field(schema, 'th_rehydrated'))\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "new_schema = create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gcsfs import GCSFileSystem\n",
    "\n",
    "new_schema = create_schema()\n",
    "fs = GCSFileSystem(project='trollhunters')\n",
    "\n",
    "with fs.open('gs://spain-tweets/schemas/tweet-clean.pickle', 'wb') as f:\n",
    "    pickle.dump(new_schema, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
